# CS229 课程讲义中文翻译

| 原作者 | 翻译 | 校对 |
| --- | --- | --- |
| [Andrew Ng  吴恩达](http://www.andrewng.org/) | [CycleUser](https://www.zhihu.com/people/cycleuser/columns) | [XiaoDong_Wang](https://github.com/Dongzhixiao) |


|相关链接|
|---|
|[Github 地址](https://github.com/Kivy-CN/Stanford-CS-229-CN)|
|[知乎专栏](https://zhuanlan.zhihu.com/MachineLearn)|
|[斯坦福大学 CS229 课程网站](http://cs229.stanford.edu/)|
|[网易公开课中文字幕视频](http://open.163.com/movie/2008/1/M/C/M6SGF6VB4_M6SGHFBMC.html)|


# 第二章

### 第四部分 生成学习算法(Generative Learning algorithms)

目前为止，我们讲过的学习算法的模型都是$p (y|x;\theta)$，也就是给定 $x$ 下 $y$ 的条件分布，以  $\theta$  为参数。例如，逻辑回归中就是以 $h_\theta(x) = g(\theta^T x)$ 作为 $p (y|x;\theta)$ 的模型，这里的 $g$ 是一个 $S$型函数（sigmoid function）。接下来，咱们要讲一下一种不同类型的学习算法。

设想有这样一种分类问题，我们要学习基于一个动物的某个特征来辨别它是大象$(y=1)$还是小狗$(y=0)$。给定一个训练集，用逻辑回归或者基础版的**感知器算法（perceptron algorithm）** 这样的一个算法能找到一条直线，作为区分开大象和小狗的边界。接下来，要辨别一个新的动物是大象还是小狗，程序就要检查这个新动物的值落到了划分出来的哪个区域中，然后根据所落到的区域来给出预测。

还有另外一种方法。首先，观察大象，然后我们针对大象的样子来进行建模。然后，再观察小狗，针对小狗的样子另外建立一个模型。最后要判断一种新动物归属哪一类，我们可以把新动物分别用大象和小狗的模型来进比对，看看新动物更接近哪个训练集中已有的模型。

例如逻辑回归之类的直接试图建立 $p(y|x)$的算法，以及感知器算法（perceptron algorithm）等直接用投图（mappings directly）的思路来判断对应 $X$ 的值落到了 $\{0, 1\}$ 中哪个区域的算法，这些都叫**判别式学习算法（discriminative learning algorithms）。** 和之前的这些判别式算法不同，下面我们要讲的新算法是对 $p(x|y)$ 和 $p(y)$来进行建模。这类算法叫**做生成学习算法（generative learning algorithms）**。例如如果 $y$ 用来表示一个样例是  小狗 $(0)$ 或者  大象 $(1)$，那么$p(x|y = 0)$就是对小狗特征分布的建模，而$p(x|y = 1)$就是对大象特征分布的建模。

对 $p(y)$ (通常称为**class priors**`译者注：这里没有找到合适的词进行翻译`) 和$p(x|y)$ 进行建模之后，我们的算法就是用**贝叶斯规则（Bayes rule）** 来推导对应给定 $x$ 下 $y$ 的**后验分布（posterior distribution）**：

$$
p(y|x)=\frac{p(x|y)p(y)}{p(x)}
$$

这里的**分母（denominator）** 为：$p(x) = p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)$（这个等式关系可以根据概率的标准性质来推导验证`译者注：其实就是条件概率`），这样接下来就可以把它表示成我们熟悉的 $p(x|y)$和 $p(y)$ 的形式了。实际上如果我们计算$p(y|x)$ 来进行预测，那就并不需要去计算这个分母，因为有下面的等式关系：

$$\begin{aligned}
\arg \max_y p(y|x) & =\arg \max_y \frac{p(x|y)p(y)}{p(x)}\\
&= \arg \max_y p(x|y)p(y)
\end{aligned}$$

#### 1 高斯判别分析（Gaussian discriminant analysis）

咱们要学的第一个生成学习算法就是**高斯判别分析（Gaussian discriminant analysis** ，缩写为GDA。`译者注：高斯真棒！`）在这个模型里面，我们**假设 $p(x|y)$是一个多元正态分布。** 所以首先咱们简单讲一下多元正态分布的一些特点，然后再继续讲 GDA 高斯判别分析模型。

##### 1.1 多元正态分布（multivariate normal distribution）

$n$维多元正态分布，也叫做多变量高斯分布，参数为一个$n$维 **均值向量** $\mu \in  R^n$，以及一个 **协方差矩阵** $\Sigma \in  R^{n\times n}$，其中$\Sigma \geq 0$ 是一个对称（symmetric）的半正定（positive semi-definite）矩阵。当然也可以写成"$N (\mu, \Sigma)$" 的分布形式，密度（density）函数为：

$$
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
$$

在上面的等式中，"$|\Sigma|$"的意思是矩阵$\Sigma$的行列式（determinant）。对于一个在 $N(\mu,\Sigma)$分布中的随机变量 $X$ ，其平均值（跟正态分布里面差不多，所以并不意外）就是 $\mu$ 了：

$$
E[X]=\int_x xp(x;\mu,\Sigma)dx=\mu
$$

随机变量$Z$是一个有值的向量（vector-valued random variable），$Z$ 的 **协方差（covariance）** 的定义是：$Cov(Z) = E[(Z-E[Z])(Z-E[Z])^T ]$。这是对实数随机变量的方差（variance）这一概念的泛化扩展。这个协方差还可以定义成$Cov(Z) = E[ZZ^T]-(E[Z])(E[Z])^T$（你可以自己证明一下这两个定义实际上是等价的。）如果 $X$ 是一个多变量正态分布，即 $X \sim N (\mu, \Sigma)$，则有：

$$
Cov(X)=\Sigma
$$

下面这些样例是一些高斯分布的密度图，如下图所示：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f1.png)

最左边的图，展示的是一个均值为$0$（实际上是一个$2\times 1$ 的零向量）的高斯分布，协方差矩阵就是$\Sigma = I$ （一个 $2\times 2$的单位矩阵，identity matrix）。这种均值为$0$ 并且协方差矩阵为单位矩阵的高斯分布也叫做**标准正态分布。** 中间的图中展示的是均值为$0$而协方差矩阵是$0.6I$ 的高斯分布的概率密度函数；最右边的展示的是协方差矩阵$\Sigma = 2I$的高斯分布的概率密度函数。从这几个图可以看出，随着协方差矩阵$\Sigma$变大，高斯分布的形态就变得更宽平（spread-out），而如果协方差矩阵$\Sigma$变小，分布就会更加集中（compressed）。

来看一下更多的样例：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f2.png)

上面这几个图展示的是均值为$0$，但协方差矩阵各不相同的高斯分布，其中的协方差矩阵依次如下所示：

$$
\Sigma =\begin{bmatrix} 
1 & 0 \\ 0 & 1  \end{bmatrix};
\Sigma =\begin{bmatrix} 
1 & 0.5 \\ 0.5 & 1 
\end{bmatrix};
\Sigma =\begin{bmatrix} 
1 & 0.8 \\ 0.8 & 1
\end{bmatrix}
$$

第一幅图还跟之前的标准正态分布的样子很相似，然后我们发现随着增大协方差矩阵$\Sigma$ 的反对角线（off-diagonal）的值，密度图像开始朝着  45° 方向 (也就是 $x_1 = x_2$ 所在的方向)逐渐压缩（compressed）。  看一下三个同样分布密度图的轮廓图（contours）能看得更明显：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f3.png)

下面的是另外一组样例，调整了协方差矩阵$\Sigma$:

$$
\Sigma =\begin{bmatrix} 
1 & 0.5 \\ 0.5 & 1 
\end{bmatrix};
\Sigma =\begin{bmatrix} 
1 & 0.8 \\ 0.8 & 1 
\end{bmatrix}
\Sigma =\begin{bmatrix} 
3 & 0.8 \\ 0.8 & 1 \end{bmatrix};
$$

上面这三个图像对应的协方差矩阵分别如下所示：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f4.png)

从最左边的到中间`译者注：注意，左边和中间的这两个协方差矩阵中，右上和左下的元素都是负值！`很明显随着协方差矩阵中右上左下这个对角线方向元素的值的降低，图像还是又被压扁了（compressed），只是方向是反方向的。最后，随着我们修改参数，通常生成的轮廓图（contours）都是椭圆（最右边的图就是一个例子）。

再举一些例子，固定协方差矩阵为单位矩阵，即$\Sigma = I$，然后调整均值$\mu$，我们就可以让密度图像随着均值而移动：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f5.png)

上面的图像中协方差矩阵都是单位矩阵，即 $\Sigma = I$，对应的均值$\mu$如下所示：

$$
\mu =\begin{bmatrix} 
1 \\ 0
\end{bmatrix};
\mu =\begin{bmatrix} 
-0.5 \\ 0
\end{bmatrix};
\mu =\begin{bmatrix} 
-1 \\ -1.5
\end{bmatrix};
$$

##### 1.2 高斯判别分析模型（Gaussian Discriminant Analysis model）

假如我们有一个分类问题，其中输入特征 $x$ 是一系列的连续随机变量（continuous-valued random variables），那就可以使用高斯判别分析（Gaussian Discriminant Analysis ，缩写为 GDA）模型，其中对 $p(x|y)$用多元正态分布来进行建模。这个模型为：

$$
\begin{aligned}
y & \sim Bernoulli(\phi)\\
x|y = 0 & \sim N(\mu_o,\Sigma)\\
x|y = 1 & \sim N(\mu_1,\Sigma)\\
\end{aligned}
$$

分布写出来的具体形式如下：

$$
\begin{aligned}
p(y) & =\phi^y (1-\phi)^{1-y}\\
p(x|y=0) & = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp ( - \frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)  )\\
p(x|y=1) & = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp ( - \frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)  )\\
\end{aligned}
$$

在上面的等式中，模型的参数包括$\phi$, $\Sigma$, $\mu_0$ 和$\mu_1$。（要注意，虽然这里有两个不同方向的均值向量$\mu_0$ 和 $\mu_1$，针对这个模型还是一般只是用一个协方差矩阵$\Sigma$。）取对数的似然函数（log-likelihood）如下所示：

$$
\begin{aligned}
l(\phi,\mu_0,\mu_1,\Sigma) &= \log \prod^m_{i=1}p(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\
&= \log \prod^m_{i=1}p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)\\
\end{aligned}
$$

通过使 $l$ 取得最大值，找到对应的参数组合，然后就能找到该参数组合对应的最大似然估计，如下所示（参考习题集1）：

$$
\begin{aligned}
\phi & = \frac {1}{m} \sum^m_{i=1}1\{y^{(i)}=1\}\\
\mu_0 & = \frac{\sum^m_{i=1}1\{y^{(i)}=0\}x^{(i)}}{\sum^m_{i=1}1\{y^{(i)}=0\}}\\
\mu_1 & = \frac{\sum^m_{i=1}1\{y^{(i)}=1\}x^{(i)}}{\sum^m_{i=1}1\{y^{(i)}=1\}}\\
\Sigma & = \frac{1}{m}\sum^m_{i=1}(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T\\
\end{aligned}
$$

用图形化的方式来表述，这个算法可以按照下面的图示所表示：

![](https://raw.githubusercontent.com/Kivy-CN/Stanford-CS-229-CN/master/img/cs229note2f6.png)

图中展示的点就是训练数据集，图中的两个高斯分布就是针对两类数据各自进行的拟合。要注意这两个高斯分布的轮廓图有同样的形状和拉伸方向，这是因为他们都有同样的协方差矩阵$\Sigma$，但他们有不同的均值$\mu_0$ 和 $\mu_1$ 。此外，图中的直线给出了$p (y = 1|x) = 0.5$ 这条边界线。在这条边界的一侧，我们预测 $y = 1$是最可能的结果，而另一侧，就预测 $y = 0$是最可能的结果。

##### 1.3 讨论：高斯判别分析（GDA）与逻辑回归（logistic regression）

高斯判别分析模型与逻辑回归有很有趣的相关性。如果我们把变量（quantity）$p (y = 1|x; \phi, \mu_0, \mu_1, \Sigma)$ 作为一个 $x$ 的函数，就会发现可以用如下的形式来表达：

$$
p(y=1|x;\phi,\Sigma,\mu_0,\mu_1)=\frac 1 {1+exp(-\theta^Tx)}
$$

其中的  $\theta$  是对$\phi$, $\Sigma$, $\mu_0$, $\mu_1$的某种函数。这就是逻辑回归（也是一种判别分析算法）用来对$p (y = 1|x)$ 建模的形式。

> 注：上面这里用到了一种转换，就是重新对$x^{(i)}$向量进行了定义，在右手侧（right-hand-side）增加了一个额外的坐标$x_0^{(i)} = 1$，然后使之成为了一个 $n+1$维的向量；具体内容参考习题集1。

这两个模型中什么时候该选哪一个呢？一般来说，高斯判别分析（GDA）和逻辑回归，对同一个训练集，可能给出的判别曲线是不一样的。哪一个更好些呢？

我们刚刚已经表明，如果$p(x|y)$是一个多变量的高斯分布（且具有一个共享的协方差矩阵$\Sigma$），那么$p(y|x)$则必然符合一个逻辑函数（logistic function）。然而，反过来，这个命题是不成立的。例如假如$p(y|x)$是一个逻辑函数，这并不能保证$p(x|y)$一定是一个多变量的高斯分布。这就表明**高斯判别模型能比逻辑回归对数据进行更强的建模和假设（stronger modeling assumptions）。** 这也就意味着，**在这两种模型假设都可用的时候，高斯判别分析法去拟合数据是更好的，是一个更好的模型。** 尤其当$p(x|y)$已经确定是一个高斯分布（有共享的协方差矩阵$\Sigma$），那么高斯判别分析是**渐进有效的（asymptotically efficient）。** 实际上，这也意味着，在面对非常大的训练集（训练样本规模 $m$特别大）的时候，严格来说，可能就没有什么别的算法能比高斯判别分析更好（比如考虑到对 $p(y|x)$估计的准确度等等）。所以在这种情况下就表明，高斯判别分析（GDA）是一个比逻辑回归更好的算法；再扩展一下，即便对于小规模的训练集，我们最终也会发现高斯判别分析（GDA）是更好的。

奈何事有正反，由于逻辑回归做出的假设要明显更弱一些（significantly weaker），所以因此逻辑回归给出的判断鲁棒性（robust）也更强，同时也对错误的建模假设不那么敏感。有很多不同的假设集合都能够将$p(y|x)$引向逻辑回归函数。例如，如果$x|y = 0\sim Poisson(\lambda_0)$ 是一个泊松分布，而$x|y = 1\sim Poisson(\lambda_1)$也是一个泊松分布，那么$p(y|x)$也将是适合逻辑回归的（logistic）。逻辑回归也适用于这类的泊松分布的数据。但对这样的数据，如果我们强行使用高斯判别分析（GDA），然后用高斯分布来拟合这些非高斯数据，那么结果的可预测性就会降低，而且GDA这种方法也许可行，也有可能是不能用。

总结一下也就是：高斯判别分析方法（GDA）能够建立更强的模型假设，并且在数据利用上更加有效（比如说，需要更少的训练集就能有"还不错的"效果），当然前提是模型假设争取或者至少接近正确。逻辑回归建立的假设更弱，因此对于偏离的模型假设来说更加鲁棒（robust）。然而，如果训练集数据的确是非高斯分布的（non-Gaussian），而且是有限的大规模数据（in the limit of large datasets），那么逻辑回归几乎总是比GDA要更好的。因此，在实际中，逻辑回归的使用频率要比GDA高得多。（关于判别和生成模型的对比的相关讨论也适用于我们下面要讲的朴素贝叶斯算法（Naive Bayes），但朴素贝叶斯算法还是被认为是一个非常优秀也非常流行的分类算法。）
